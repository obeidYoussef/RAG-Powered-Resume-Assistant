{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa15d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project_ML\\LLMs\\RAG-powered Assistant\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "import gradio as gr\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a93c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"Documents\")\n",
    "doc_before_split = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5332ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_before_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3edb7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_before_split[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8036905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176\n",
      "219\n",
      "771\n",
      "704\n",
      "705\n",
      "651\n",
      "613\n",
      "600\n",
      "633\n",
      "629\n",
      "620\n",
      "632\n",
      "871\n",
      "0\n",
      "1395\n",
      "1053\n",
      "691\n",
      "661\n",
      "646\n",
      "555\n",
      "1047\n",
      "97\n",
      "809\n",
      "802\n"
     ]
    }
   ],
   "source": [
    "for doc in doc_before_split:\n",
    "    print(len(doc.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a7a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_before_split = [doc for doc in doc_before_split if len(doc.page_content) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ed976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100,\n",
    ")\n",
    "\n",
    "docs_after_split = text_splitter.split_documents(doc_before_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "288d1d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "290\n"
     ]
    }
   ],
   "source": [
    "print(len(docs_after_split))\n",
    "print(len(docs_after_split[1].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1306a5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average document length before split: 690.8333333333334\n",
      "Average document length after split: 624.1851851851852\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs]) / len(docs)\n",
    "print(f\"Average document length before split: {avg_doc_length(doc_before_split)}\")\n",
    "print(f\"Average document length after split: {avg_doc_length(docs_after_split)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f964a598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell i5\\AppData\\Local\\Temp\\ipykernel_23772\\2152875966.py:1: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  huggingface_embedding = HuggingFaceBgeEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "huggingface_embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs = {\"device\": \"cpu\"},\n",
    "    encode_kwargs = {\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc21134",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_vectorizer = FAISS.from_documents(docs_after_split, huggingface_embedding)\n",
    "\n",
    "chroma_vectorizer = Chroma.from_documents(\n",
    "    documents = docs_after_split, \n",
    "    embedding = huggingface_embedding, \n",
    "    persist_directory = \"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0403133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Find candidates with more than 2 years of experience in data science and machine learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58f39f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_retriever = faiss_vectorizer.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "chroma_retriever = chroma_vectorizer.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"xxxxxxxxxxxxxxxxxxxx\" # put you api key here\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-3-flash-preview\",\n",
    "    temperature=1.0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3235218",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an AI assistant specialized in analyzing resumes. \n",
    "Use the following pieces of context (resume content) to answer the question at the end. \n",
    "Follow these rules carefully:\n",
    "\n",
    "1. If the answer is not present in the context, do NOT guess. Say: \"I couldn't find this information in the resumes provided.\"\n",
    "2. Extract skills, experience, and relevant qualifications concisely.\n",
    "3. Provide the answer in a maximum of 5 sentences.\n",
    "4. Whenever possible, include the name of the candidate and the source resume file.\n",
    "\n",
    "Resume Content:\n",
    "{context}\n",
    "\n",
    "Query: {question}\n",
    "\n",
    "Answer (concise and structured):\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, input_variables = [\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ec896b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm = model, \n",
    "    chain_type = \"stuff\",\n",
    "    retriever = faiss_retriever,\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs = {\"prompt\" : PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b681228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm = model, \n",
    "    chain_type = \"stuff\",\n",
    "    retriever = chroma_retriever,\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs = {\"prompt\" : PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce2faff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Find candidates with more than 2 years of experience in data science and machine learning', 'result': 'Based on the resumes provided, all four candidates meet the criteria:\\n\\n1. **Sara Haddad** has 3 years of experience as a Data Scientist specializing in machine learning models (XGBoost, Random Forest) and statistical analysis.\\n2. **Fadi Karam** brings 7 years of experience as a Senior Data Engineer, focusing on designing AI pipelines and infrastructure to support machine learning workflows.\\n3. **Nour El Din** has 3 years of experience as an AI Software Engineer, with expertise in integrating machine learning models into scalable APIs and backend systems.\\n4. **Reem Khalaf** possesses 4 years of experience as an AI Healthcare Analyst, applying machine learning and predictive modeling to medical datasets.\\n5. All candidates demonstrate proficiency in Python and SQL across their respective data science and AI-focused roles.', 'source_documents': [Document(id='e5a420ea-46e0-43f9-89b1-952330aa6a27', metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2026-01-04T12:33:45+00:00', 'author': 'Youssef Obeid', 'moddate': '2026-01-04T12:33:45+00:00', 'source': 'Documents\\\\Resume 2.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Sara Haddad \\nData Scientist \\nTripoli, Lebanon \\nEmail: sara.haddad.ds@gmail.com \\nSUMMARY \\nData Scientist with 3 years of experience transforming raw data into actionable \\ninsights using statistical analysis, machine learning models, and visualization \\ntechniques. \\nAREAS OF EXPERTISE \\nData Analysis, Statistics, Machine Learning, Data Visualization, Python, SQL, EDA, \\nFeature Engineering \\nEDUCATION \\nUniversity of Balamand \\nBSc in Computer Science \\n2019 – 2022 \\nPROJECTS \\nSales Forecasting System \\n\\uf0b7 Built XGBoost and Random Forest models \\n\\uf0b7 Improved sales prediction accuracy \\nCustomer Segmentation \\n\\uf0b7 Applied KMeans clustering \\n\\uf0b7 Visualized clusters using PCA \\nPROFESSIONAL EXPERIENCE \\nData Scientist \\nInsight Analytics — Tripoli \\n2022 – Present \\nSKILLS \\nPython, SQL, Pandas, NumPy, Scikit-learn, Power BI \\nLANGUAGES \\nArabic (Native), English (Advanced), French (Basic)'), Document(id='1a4e0038-056f-4712-a941-8ae608cc2d8b', metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2026-01-04T12:35:21+00:00', 'author': 'Youssef Obeid', 'moddate': '2026-01-04T12:35:21+00:00', 'source': 'Documents\\\\Resume 9.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Fadi Karam \\nData Engineer (AI Pipelines) \\nBeirut, Lebanon \\nEmail: fadi.karam.de@gmail.com \\nSUMMARY \\nData Engineer with 7 years of experience designing scalable data pipelines and \\ninfrastructure to support machine learning workflows. \\nAREAS OF EXPERTISE \\nData Engineering, ETL Pipelines, Big Data, SQL, Python, Data Warehousing, ML \\nSupport Systems \\nEDUCATION \\nAmerican University of Beirut \\nBSc in Computer Engineering \\n2013 – 2017 \\nPROJECTS \\nML Data Pipeline Architecture \\n\\uf0b7 Designed ETL pipelines for AI model training \\nData Warehouse for Analytics \\n\\uf0b7 Optimized data ingestion and querying performance \\nPROFESSIONAL EXPERIENCE \\nSenior Data Engineer \\nDataFlow Systems — Beirut \\n2017 – Present \\nSKILLS \\nPython, SQL, Airflow, PostgreSQL, Data Warehousing \\nLANGUAGES \\nArabic (Native), English (Advanced)'), Document(id='0dc4cd6c-ba3b-4d2a-8aa2-c5bdcffb4964', metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2026-01-04T12:35:09+00:00', 'author': 'Youssef Obeid', 'moddate': '2026-01-04T12:35:09+00:00', 'source': 'Documents\\\\Resume 8.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Nour El Din \\nAI Software Engineer \\nZahle, Lebanon \\nEmail: nour.eldin.ai@gmail.com \\nGitHub: https://github.com/noureldin-ai \\nSUMMARY \\nAI Software Engineer with 3 years of experience integrating machine learning models \\ninto scalable software systems and APIs. \\nAREAS OF EXPERTISE \\nAI Integration, Software Development, REST APIs, Machine Learning, Python, Backend \\nDevelopment \\nEDUCATION \\nRafik Hariri University \\nBSc in Computer Science \\n2018 – 2022 \\nPROJECTS \\nRecommendation Engine API \\n\\uf0b7 Integrated collaborative filtering models into backend services \\nAI Chatbot Backend \\n\\uf0b7 Built RESTful APIs for ML-driven chatbot logic \\nPROFESSIONAL EXPERIENCE \\nAI Software Engineer \\nTechBridge — Bekaa \\n2022 – Present \\nSKILLS \\nPython, Flask, FastAPI, SQL, Machine Learning \\nLANGUAGES \\nArabic (Native), English (Advanced)'), Document(id='24227ad2-2212-4336-b9fb-c5d2f0425fa5', metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2026-01-04T12:36:28+00:00', 'author': 'Youssef Obeid', 'moddate': '2026-01-04T12:36:28+00:00', 'source': 'Documents\\\\Resume 16.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Reem Khalaf \\nAI Healthcare Analyst \\nTripoli, Lebanon \\nEmail: reem.khalaf.ai@gmail.com \\nSUMMARY \\nAI Healthcare Analyst with 4 years of experience applying machine learning \\ntechniques to medical and healthcare datasets. \\nAREAS OF EXPERTISE \\nHealthcare Analytics, Predictive Modeling, Python, Data Analysis \\nEDUCATION \\nLebanese University \\nBSc in Computer Science \\n2016 – 2020 \\nPROJECTS \\n\\uf0b7 Patient Risk Prediction Model \\n\\uf0b7 Medical Data Visualization Dashboard \\nPROFESSIONAL EXPERIENCE \\nAI Healthcare Analyst \\nMedTech Analytics — Tripoli \\n2020 – Present \\nSKILLS \\nPython, Pandas, ML Models \\nLANGUAGES \\nArabic (Native), English (Advanced)')]}\n"
     ]
    }
   ],
   "source": [
    "faiss_result = faiss_retrievalQA.invoke({\"query\" : query})\n",
    "print(faiss_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c417c8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the resumes provided, all four candidates meet the criteria:\\n\\n1. **Sara Haddad** has 3 years of experience as a Data Scientist specializing in machine learning models (XGBoost, Random Forest) and statistical analysis.\\n2. **Fadi Karam** brings 7 years of experience as a Senior Data Engineer, focusing on designing AI pipelines and infrastructure to support machine learning workflows.\\n3. **Nour El Din** has 3 years of experience as an AI Software Engineer, with expertise in integrating machine learning models into scalable APIs and backend systems.\\n4. **Reem Khalaf** possesses 4 years of experience as an AI Healthcare Analyst, applying machine learning and predictive modeling to medical datasets.\\n5. All candidates demonstrate proficiency in Python and SQL across their respective data science and AI-focused roles.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fe32153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Find candidates with more than 2 years of experience in data science and machine learning', 'result': 'Sara Haddad (Data Scientist resume) has 3 years of experience in data science and machine learning, specializing in statistical analysis and predictive modeling. Her qualifications include a BSc in Computer Science and expertise in Python, SQL, Scikit-learn, and Power BI. She has successfully implemented machine learning projects using XGBoost, Random Forest, and KMeans clustering for sales forecasting and customer segmentation. Currently, she works as a Data Scientist at Insight Analytics, where she focuses on transforming raw data into actionable insights.', 'source_documents': [Document(metadata={'producer': 'www.ilovepdf.com', 'source': 'Documents\\\\Resume 2.pdf', 'moddate': '2026-01-04T12:33:45+00:00', 'page': 0, 'page_label': '1', 'total_pages': 2, 'creationdate': '2026-01-04T12:33:45+00:00', 'creator': 'Microsoft® Word 2016', 'author': 'Youssef Obeid'}, page_content='Sara Haddad \\nData Scientist \\nTripoli, Lebanon \\nEmail: sara.haddad.ds@gmail.com \\nSUMMARY \\nData Scientist with 3 years of experience transforming raw data into actionable \\ninsights using statistical analysis, machine learning models, and visualization \\ntechniques. \\nAREAS OF EXPERTISE \\nData Analysis, Statistics, Machine Learning, Data Visualization, Python, SQL, EDA, \\nFeature Engineering \\nEDUCATION \\nUniversity of Balamand \\nBSc in Computer Science \\n2019 – 2022 \\nPROJECTS \\nSales Forecasting System \\n\\uf0b7 Built XGBoost and Random Forest models \\n\\uf0b7 Improved sales prediction accuracy \\nCustomer Segmentation \\n\\uf0b7 Applied KMeans clustering \\n\\uf0b7 Visualized clusters using PCA \\nPROFESSIONAL EXPERIENCE \\nData Scientist \\nInsight Analytics — Tripoli \\n2022 – Present \\nSKILLS \\nPython, SQL, Pandas, NumPy, Scikit-learn, Power BI \\nLANGUAGES \\nArabic (Native), English (Advanced), French (Basic)'), Document(metadata={'source': 'Documents\\\\Resume 2.pdf', 'moddate': '2026-01-04T12:33:45+00:00', 'creationdate': '2026-01-04T12:33:45+00:00', 'page_label': '1', 'author': 'Youssef Obeid', 'total_pages': 2, 'page': 0, 'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016'}, page_content='Sara Haddad \\nData Scientist \\nTripoli, Lebanon \\nEmail: sara.haddad.ds@gmail.com \\nSUMMARY \\nData Scientist with 3 years of experience transforming raw data into actionable \\ninsights using statistical analysis, machine learning models, and visualization \\ntechniques. \\nAREAS OF EXPERTISE \\nData Analysis, Statistics, Machine Learning, Data Visualization, Python, SQL, EDA, \\nFeature Engineering \\nEDUCATION \\nUniversity of Balamand \\nBSc in Computer Science \\n2019 – 2022 \\nPROJECTS \\nSales Forecasting System \\n\\uf0b7 Built XGBoost and Random Forest models \\n\\uf0b7 Improved sales prediction accuracy \\nCustomer Segmentation \\n\\uf0b7 Applied KMeans clustering \\n\\uf0b7 Visualized clusters using PCA \\nPROFESSIONAL EXPERIENCE \\nData Scientist \\nInsight Analytics — Tripoli \\n2022 – Present \\nSKILLS \\nPython, SQL, Pandas, NumPy, Scikit-learn, Power BI \\nLANGUAGES \\nArabic (Native), English (Advanced), French (Basic)'), Document(metadata={'page': 0, 'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'author': 'Youssef Obeid', 'moddate': '2026-01-04T12:35:21+00:00', 'total_pages': 1, 'creationdate': '2026-01-04T12:35:21+00:00', 'page_label': '1', 'source': 'Documents\\\\Resume 9.pdf'}, page_content='Fadi Karam \\nData Engineer (AI Pipelines) \\nBeirut, Lebanon \\nEmail: fadi.karam.de@gmail.com \\nSUMMARY \\nData Engineer with 7 years of experience designing scalable data pipelines and \\ninfrastructure to support machine learning workflows. \\nAREAS OF EXPERTISE \\nData Engineering, ETL Pipelines, Big Data, SQL, Python, Data Warehousing, ML \\nSupport Systems \\nEDUCATION \\nAmerican University of Beirut \\nBSc in Computer Engineering \\n2013 – 2017 \\nPROJECTS \\nML Data Pipeline Architecture \\n\\uf0b7 Designed ETL pipelines for AI model training \\nData Warehouse for Analytics \\n\\uf0b7 Optimized data ingestion and querying performance \\nPROFESSIONAL EXPERIENCE \\nSenior Data Engineer \\nDataFlow Systems — Beirut \\n2017 – Present \\nSKILLS \\nPython, SQL, Airflow, PostgreSQL, Data Warehousing \\nLANGUAGES \\nArabic (Native), English (Advanced)'), Document(metadata={'moddate': '2026-01-04T12:35:21+00:00', 'creator': 'Microsoft® Word 2016', 'source': 'Documents\\\\Resume 9.pdf', 'total_pages': 1, 'creationdate': '2026-01-04T12:35:21+00:00', 'author': 'Youssef Obeid', 'page': 0, 'page_label': '1', 'producer': 'www.ilovepdf.com'}, page_content='Fadi Karam \\nData Engineer (AI Pipelines) \\nBeirut, Lebanon \\nEmail: fadi.karam.de@gmail.com \\nSUMMARY \\nData Engineer with 7 years of experience designing scalable data pipelines and \\ninfrastructure to support machine learning workflows. \\nAREAS OF EXPERTISE \\nData Engineering, ETL Pipelines, Big Data, SQL, Python, Data Warehousing, ML \\nSupport Systems \\nEDUCATION \\nAmerican University of Beirut \\nBSc in Computer Engineering \\n2013 – 2017 \\nPROJECTS \\nML Data Pipeline Architecture \\n\\uf0b7 Designed ETL pipelines for AI model training \\nData Warehouse for Analytics \\n\\uf0b7 Optimized data ingestion and querying performance \\nPROFESSIONAL EXPERIENCE \\nSenior Data Engineer \\nDataFlow Systems — Beirut \\n2017 – Present \\nSKILLS \\nPython, SQL, Airflow, PostgreSQL, Data Warehousing \\nLANGUAGES \\nArabic (Native), English (Advanced)')]}\n"
     ]
    }
   ],
   "source": [
    "chroma_result = chroma_retrievalQA.invoke({\"query\" : query})\n",
    "print(chroma_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2d61986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sara Haddad (Data Scientist resume) has 3 years of experience in data science and machine learning, specializing in statistical analysis and predictive modeling. Her qualifications include a BSc in Computer Science and expertise in Python, SQL, Scikit-learn, and Power BI. She has successfully implemented machine learning projects using XGBoost, Random Forest, and KMeans clustering for sales forecasting and customer segmentation. Currently, she works as a Data Scientist at Insight Analytics, where she focuses on transforming raw data into actionable insights.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cad99341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 documents retrieved for FAISS:\n",
      "****************************************************************************************************\n",
      "Relevant Document #1\n",
      "Source file: Documents\\Resume 2.pdf, Page: 0\n",
      "Content (first 500 chars):\n",
      "Sara Haddad \n",
      "Data Scientist \n",
      "Tripoli, Lebanon \n",
      "Email: sara.haddad.ds@gmail.com \n",
      "SUMMARY \n",
      "Data Scientist with 3 years of experience transforming raw data into actionable \n",
      "insights using statistical analysis, machine learning models, and visualization \n",
      "techniques. \n",
      "AREAS OF EXPERTISE \n",
      "Data Analysis, Statistics, Machine Learning, Data Visualization, Python, SQL, EDA, \n",
      "Feature Engineering \n",
      "EDUCATION \n",
      "University of Balamand \n",
      "BSc in Computer Science \n",
      "2019 – 2022 \n",
      "PROJECTS \n",
      "Sales Forecasting System \n",
      " \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevant Document #2\n",
      "Source file: Documents\\Resume 9.pdf, Page: 0\n",
      "Content (first 500 chars):\n",
      "Fadi Karam \n",
      "Data Engineer (AI Pipelines) \n",
      "Beirut, Lebanon \n",
      "Email: fadi.karam.de@gmail.com \n",
      "SUMMARY \n",
      "Data Engineer with 7 years of experience designing scalable data pipelines and \n",
      "infrastructure to support machine learning workflows. \n",
      "AREAS OF EXPERTISE \n",
      "Data Engineering, ETL Pipelines, Big Data, SQL, Python, Data Warehousing, ML \n",
      "Support Systems \n",
      "EDUCATION \n",
      "American University of Beirut \n",
      "BSc in Computer Engineering \n",
      "2013 – 2017 \n",
      "PROJECTS \n",
      "ML Data Pipeline Architecture \n",
      " Designed ETL pipelines \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevant Document #3\n",
      "Source file: Documents\\Resume 8.pdf, Page: 0\n",
      "Content (first 500 chars):\n",
      "Nour El Din \n",
      "AI Software Engineer \n",
      "Zahle, Lebanon \n",
      "Email: nour.eldin.ai@gmail.com \n",
      "GitHub: https://github.com/noureldin-ai \n",
      "SUMMARY \n",
      "AI Software Engineer with 3 years of experience integrating machine learning models \n",
      "into scalable software systems and APIs. \n",
      "AREAS OF EXPERTISE \n",
      "AI Integration, Software Development, REST APIs, Machine Learning, Python, Backend \n",
      "Development \n",
      "EDUCATION \n",
      "Rafik Hariri University \n",
      "BSc in Computer Science \n",
      "2018 – 2022 \n",
      "PROJECTS \n",
      "Recommendation Engine API \n",
      " Integrated\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevant Document #4\n",
      "Source file: Documents\\Resume 16.pdf, Page: 0\n",
      "Content (first 500 chars):\n",
      "Reem Khalaf \n",
      "AI Healthcare Analyst \n",
      "Tripoli, Lebanon \n",
      "Email: reem.khalaf.ai@gmail.com \n",
      "SUMMARY \n",
      "AI Healthcare Analyst with 4 years of experience applying machine learning \n",
      "techniques to medical and healthcare datasets. \n",
      "AREAS OF EXPERTISE \n",
      "Healthcare Analytics, Predictive Modeling, Python, Data Analysis \n",
      "EDUCATION \n",
      "Lebanese University \n",
      "BSc in Computer Science \n",
      "2016 – 2020 \n",
      "PROJECTS \n",
      " Patient Risk Prediction Model \n",
      " Medical Data Visualization Dashboard \n",
      "PROFESSIONAL EXPERIENCE \n",
      "AI Healthcare An\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "faiss_relevant_docs = faiss_result['source_documents']\n",
    "print(f'There are {len(faiss_relevant_docs)} documents retrieved for FAISS:')\n",
    "print(\"*\" * 100)\n",
    "\n",
    "for i, doc in enumerate(faiss_relevant_docs):\n",
    "    print(f\"Relevant Document #{i+1}\")\n",
    "    print(f\"Source file: {doc.metadata.get('source', 'Unknown')}, Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(f\"Content (first 500 chars):\\n{doc.page_content[:500]}\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e9b9987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 documents retrieved for Chroma:\n",
      "****************************************************************************************************\n",
      "Relevant Document #1\n",
      "Source file: Documents\\Resume 2.pdf, Page: 0\n",
      "Content (first 500 chars):\n",
      "Sara Haddad \n",
      "Data Scientist \n",
      "Tripoli, Lebanon \n",
      "Email: sara.haddad.ds@gmail.com \n",
      "SUMMARY \n",
      "Data Scientist with 3 years of experience transforming raw data into actionable \n",
      "insights using statistical analysis, machine learning models, and visualization \n",
      "techniques. \n",
      "AREAS OF EXPERTISE \n",
      "Data Analysis, Statistics, Machine Learning, Data Visualization, Python, SQL, EDA, \n",
      "Feature Engineering \n",
      "EDUCATION \n",
      "University of Balamand \n",
      "BSc in Computer Science \n",
      "2019 – 2022 \n",
      "PROJECTS \n",
      "Sales Forecasting System \n",
      " \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevant Document #2\n",
      "Source file: Documents\\Resume 2.pdf, Page: 0\n",
      "Content (first 500 chars):\n",
      "Sara Haddad \n",
      "Data Scientist \n",
      "Tripoli, Lebanon \n",
      "Email: sara.haddad.ds@gmail.com \n",
      "SUMMARY \n",
      "Data Scientist with 3 years of experience transforming raw data into actionable \n",
      "insights using statistical analysis, machine learning models, and visualization \n",
      "techniques. \n",
      "AREAS OF EXPERTISE \n",
      "Data Analysis, Statistics, Machine Learning, Data Visualization, Python, SQL, EDA, \n",
      "Feature Engineering \n",
      "EDUCATION \n",
      "University of Balamand \n",
      "BSc in Computer Science \n",
      "2019 – 2022 \n",
      "PROJECTS \n",
      "Sales Forecasting System \n",
      " \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevant Document #3\n",
      "Source file: Documents\\Resume 9.pdf, Page: 0\n",
      "Content (first 500 chars):\n",
      "Fadi Karam \n",
      "Data Engineer (AI Pipelines) \n",
      "Beirut, Lebanon \n",
      "Email: fadi.karam.de@gmail.com \n",
      "SUMMARY \n",
      "Data Engineer with 7 years of experience designing scalable data pipelines and \n",
      "infrastructure to support machine learning workflows. \n",
      "AREAS OF EXPERTISE \n",
      "Data Engineering, ETL Pipelines, Big Data, SQL, Python, Data Warehousing, ML \n",
      "Support Systems \n",
      "EDUCATION \n",
      "American University of Beirut \n",
      "BSc in Computer Engineering \n",
      "2013 – 2017 \n",
      "PROJECTS \n",
      "ML Data Pipeline Architecture \n",
      " Designed ETL pipelines \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevant Document #4\n",
      "Source file: Documents\\Resume 9.pdf, Page: 0\n",
      "Content (first 500 chars):\n",
      "Fadi Karam \n",
      "Data Engineer (AI Pipelines) \n",
      "Beirut, Lebanon \n",
      "Email: fadi.karam.de@gmail.com \n",
      "SUMMARY \n",
      "Data Engineer with 7 years of experience designing scalable data pipelines and \n",
      "infrastructure to support machine learning workflows. \n",
      "AREAS OF EXPERTISE \n",
      "Data Engineering, ETL Pipelines, Big Data, SQL, Python, Data Warehousing, ML \n",
      "Support Systems \n",
      "EDUCATION \n",
      "American University of Beirut \n",
      "BSc in Computer Engineering \n",
      "2013 – 2017 \n",
      "PROJECTS \n",
      "ML Data Pipeline Architecture \n",
      " Designed ETL pipelines \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chroma_relevant_docs = chroma_result['source_documents']\n",
    "print(f'There are {len(chroma_relevant_docs)} documents retrieved for Chroma:')\n",
    "print(\"*\" * 100)\n",
    "\n",
    "for i, doc in enumerate(chroma_relevant_docs):\n",
    "    print(f\"Relevant Document #{i+1}\")\n",
    "    print(f\"Source file: {doc.metadata.get('source', 'Unknown')}, Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(f\"Content (first 500 chars):\\n{doc.page_content[:500]}\")\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2483d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query):\n",
    "    \"\"\"\n",
    "    Takes a user query, returns the answer from FAISS RetrievalQA and the resumes used.\n",
    "    \"\"\"\n",
    "    # Run the RAG model\n",
    "    result = faiss_retrievalQA.invoke({\"query\": query})\n",
    "    \n",
    "    # Extract the answer text\n",
    "    answer = result['result']\n",
    "    \n",
    "    # Extract the resumes used\n",
    "    docs = result['source_documents']\n",
    "    sources = \", \".join([doc.metadata.get(\"source\", \"Unknown\") for doc in docs])\n",
    "    \n",
    "    return answer, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39061570",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=answer_query,  # The function to run\n",
    "    inputs=gr.Textbox(\n",
    "        lines=2, \n",
    "        placeholder=\"Ask about candidate experience...\"\n",
    "    ), \n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Answer\", lines=10),\n",
    "        gr.Textbox(label=\"Resumes Used\", lines=10)\n",
    "    ],\n",
    "    title=\"RAG-Powered Resume Assistant\",\n",
    "    description=\"Ask questions about resumes. The AI retrieves relevant resumes and answers based on them.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3883c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
